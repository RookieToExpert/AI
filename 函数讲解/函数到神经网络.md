## 人工智能
人工智能无非就是人输入一个值，中间的黑箱子就像是人工智能，然后人工智能输出一个答案，中间就是一个神奇的函数可以实现

- 比如翻译函数

    ![alt text](./image/image-2.png)

所以人工智能的挑战，就是如何找到一个这样完美的函数，能解决无数问题。那先从如何理解函数开始。

> 符号主义是要找到一个完美的函数，联结主义则是找到一个差不多的函数。    

### 线性函数 f(x) = wx + b
- 比如这个例子能找到完美的w和b，则有完美解

    ![alt text](./image/image.png)

- 否则就是不断调整w和b，找到一个大差不差的函数

    ![alt text](./image/image-1.png)

### 非线性函数 f(x) = g(wx + b) 也叫做激活函数
> 比如f(x) = (wx + b)^2, 比如f(x) = sin(wx + b)
- 但是通常函数不会只有这么点参数，也不会靠一层激活函数就能解决，现实中可能会有非常多的参数，并且套非常多层线性变换和激活函数

    ![alt text](./image/image-3.png)

- 每多套一层激活函数，可以理解多了一层神经元，这就是一个简单的**神经网络**

- 可以看到神经网络是先从左边输入得到隐藏层，再从隐藏层得到输出层，就是神经网络的**前向传播**

    > 每层对应关系

    ![alt text](./image/image-5.png)

    ![alt text](./image/image-6.png)

- 同时，隐藏层，输入层都可以无限增加，就会构建成一个非常庞大的神经网络，也会变成一个非常复杂的函数

    ![alt text](./image/image-7.png)

- 但不论变得多复杂，无非就是有一个**数据集**输入值x和输出值y，然后通过不断调整w和b的值得到一个最接近的函数

    ![alt text](./image/image-8.png)

### 损失函数
- 是用来判断当前函数和真实数据的拟合度，AI训练就是通过不断调整w，b得到一个最小的损失函数

    ![alt text](./image/image-9.png)

- **均方误差**是损失函数的一种，找平均的误差，那均方误差则是一个关于(w,b)的函数，因为是通过调整w,b得出的损失函数

    ![alt text](./image/image-11.png)

- 这里也引入了**偏导数**的概念和**线性回归**的概念
    - 对w求偏导：看w改一点点，损失函数变化多少。
    - 对b求偏导：看b改一点点，损失函数变化多少。
    - 这个过程就是线性回归，线性回归是机器学习里最基础的“监督学习”算法，找到最优的w，b让预测值尽可能接近真实值y。
    
    ![alt text](./image/image-12.png)

### 链式法则(微积分中的复合函数)，反向传播
- **如何求偏导数**：
    - 比如拿下面的例子解释，要得到w1对于L的偏导数：
        1. w1变化一个单位，看会使a变化多少
        2. 然后看a变化一个单位，会使y(预测值)变化多少
        3. 再看y(预测值)变化一个单位，会使L变化多少
        4. 最后把三者相乘，就知道w1变化一个单位会使得L变化多少，也就是得到w1的偏导数
- 这个过程通常是从右到左依次求导，其实也就是**反向传播**

    ![alt text](./image/image-16.png)

    ![alt text](./image/image-15.png)

### 梯度下降
- **梯度下降**则是让w和b不断往偏导数的反方向去变化去减小损失函数

    ![alt text](./image/image-13.png)

- 可以增加一个系数**学习率**去进一步增加变化的快慢

    ![alt text](./image/image-14.png)


### 模型优化方法
- 但是过度追求损失函数为0可能会出现的问题(图中蓝点表示非训练数据)：
    - **过拟合**
    - **泛化能力**(指在非训练数据以外的其他数据模型的表现能力)差
    - **鲁棒性(Robostness)**差(指模型因数据的一点点小的变化而对结果产生很大的变化，可以通过**数据增强解决**)
    ![alt text](./image/image-17.png)

- 解决办法：
    - 简化模型复杂度
    - 增大训练数据量或通过**数据增强**的办法，也就是在原有数据中进行更改，比如图片的话就是旋转，翻转，裁减等等
    - **正则化：**也可以通过定义新的损失函数，将原本的w值也加进来，避免一个参数过大导致过拟合

        ![alt text](./image/image-18.png)

    - **Dropout**：为了进一步防止过度依赖某个参数，可以在训练中随机丢弃一部分参数，从而让模型学会依赖其他参数

- 还会有其他的问题：
    - **梯度消失**：梯度太小，参数几乎不更新，训练停滞
        - 解决办法：**残差网络 (ResNet)**:在网络里加“捷径连接”，避免梯度在深层网络里消失
    - **梯度爆炸**：梯度太大，参数更新过猛，训练发散
        - 解决办法：**梯度裁剪 (Gradient Clipping)**:限制梯度的最大值，防止更新过大
    - **收敛速度慢**：参数更新太慢，需要很多轮迭代
        - 解决办法：**权重初始化 + 归一化 (BatchNorm/LayerNorm)**:给参数合理初始值 & 让输入分布稳定，加快训练
    - **计算开销大**：数据量太大，整批训练耗时
        - 解决办法：**mini-batch**:每次用一小批数据更新，兼顾效率和稳定性
    - **容易震荡/不稳定**：参数更新方向来回抖动，收敛不稳
        - 解决办法：**动量法 / RMSProp / Adam**:在梯度更新中加“惯性”或自适应学习率，走得更稳更快

